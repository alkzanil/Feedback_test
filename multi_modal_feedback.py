# -*- coding: utf-8 -*-
"""multi_modal_feedback.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ahVZSd4LXjGW-fbRvhQY6TUMDSgVtn_i
"""



from google.colab import drive
drive.mount('/content/drive')

import numpy as np # linear algebra
import pandas as pd
import math


import seaborn as sns
from matplotlib import pyplot as plt

import cv2

# Set the path to your video folder
folder_path = '/content/drive/MyDrive/Dessertation/videos'





import os
import cv2

# Set the path to your video folder
folder_path = '/content/drive/MyDrive/Dessertation/videos'

# Count the total number of video files
video_files = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.flv', '.mov'))]  # Add more extensions if needed
file_count = len(video_files)
print(f"There are {file_count} video files in the folder.\n")

# Sort and select the top 10 video files
video_files = sorted(video_files)[:10]  # Sort alphabetically and take the top 10

# Get the duration of each video
video_durations = []
for video_file in video_files:
    video_path = os.path.join(folder_path, video_file)
    cap = cv2.VideoCapture(video_path)

    if cap.isOpened():
        # Get video frame count and frames per second (fps)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)

        # Calculate duration in seconds
        duration = frame_count / fps if fps > 0 else 0
        video_durations.append((video_file, duration))

    cap.release()

# Print the durations of the top 10 videos
for i, (video_file, duration) in enumerate(video_durations, 1):
    print(f"Video {i}: {video_file} - Duration: {duration:.2f} seconds")

valid_videos = []
for video_file in video_files:
    video_path = os.path.join(folder_path, video_file)
    cap = cv2.VideoCapture(video_path)
    if cap.isOpened():
        valid_videos.append(video_file)
    cap.release()

# Update video_files with only valid videos
video_files = valid_videos
print(f"Valid video files: {len(video_files)}")

metadata = []
for video_file in video_files:
    parts = os.path.splitext(video_file)[0].split('_')
    if len(parts) == 4:  # Ensure valid metadata structure
        metadata.append({'actor': parts[0], 'scenario': parts[1], 'emotion': parts[2], 'intensity': parts[3]})
    else:
        print(f"Invalid metadata in file: {video_file}")

print(f"Valid metadata entries: {len(metadata)}")

# List all video files in the directory
video_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('mp4','flv','mov'))]

print(f"Total number of video files: {len(video_files)}")

# Assuming the naming convention: ActorID_Scenario_Emotion_Intensity.extension
# Example: 1001_DFA_ANG_XX.flv
data_records = []
for f in video_files:
    # Remove extension and split by underscore
    base_name = os.path.splitext(f)[0]
    parts = base_name.split('_')

    # Make sure we have the right number of parts
    if len(parts) == 4:
        actor_id, scenario, emotion, intensity = parts
    else:
        # If naming differs, adjust parsing logic accordingly
        actor_id = parts[0] if len(parts) > 0 else None
        scenario = parts[1] if len(parts) > 1 else None
        emotion = parts[2] if len(parts) > 2 else None
        intensity = parts[3] if len(parts) > 3 else None

    data_records.append({
        'filename': f,
        'actor_id': actor_id,
        'scenario': scenario,
        'emotion': emotion,
        'intensity': intensity
    })

# Create a DataFrame
df = pd.DataFrame(data_records)
df.head()

# @title emotion vs intensity

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['intensity'].value_counts()
    for x_label, grp in df.groupby('emotion')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('emotion')
_ = plt.ylabel('intensity')

# @title intensity

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('intensity').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title emotion

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('emotion').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Count how many videos per emotion
emotion_counts = df['emotion'].value_counts()
print("Number of videos per emotion:")
print(emotion_counts)

# Count how many videos per scenario
scenario_counts = df['scenario'].value_counts()
print("\nNumber of videos per scenario:")
print(scenario_counts)

# Count how many videos per intensity
intensity_counts = df['intensity'].value_counts()
print("\nNumber of videos per intensity:")
print(intensity_counts)

plt.figure(figsize=(8, 6))
sns.countplot(x='emotion', data=df, order=df['emotion'].value_counts().index)
plt.title('Distribution of Videos by Emotion')
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x='scenario', data=df, order=df['scenario'].value_counts().index)
plt.title('Distribution of Videos by Scenario')
plt.xlabel('Scenario')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x='intensity', data=df, order=df['intensity'].value_counts().index)
plt.title('Distribution of Videos by Intensity')
plt.xlabel('Intensity')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='emotion', hue='intensity', data=df)
plt.title('Videos by Emotion and Intensity')
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

actor_counts = df['actor_id'].value_counts()

plt.figure(figsize=(10, 6))
actor_counts.plot(kind='hist', bins=20)
plt.title('Distribution of Videos per Actor')
plt.xlabel('Number of Videos')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

!pip install moviepy==2.0.0.dev2

# Create a list of video file paths
file_paths = [os.path.join(folder_path, f)
              for f in os.listdir(folder_path)
              if f.lower().endswith(('.mp4', '.flv', '.mov'))]

print("Total number of video files:", len(file_paths))





import moviepy.editor as mp

vfc = mp.VideoFileClip(file_paths[-3])
mp.ipython_display(vfc)

!pip install decord

#Extract frames and audio
import decord

filepath = file_paths[-10]
print(filepath)
audio, vid1 = decord.AVReader(filepath, sample_rate=22050)[:]
np.concatenate([a.asnumpy()[0] for a in audio if len(a)>0]).shape[0]/22050, vid1.shape[0]/30

!pip install deepface
from deepface import DeepFace
import matplotlib.pyplot as plt
import pandas as pd

#Extract faces from video
from deepface import DeepFace
import matplotlib.pyplot as plt
import pandas as pd

print(DeepFace.__version__)

!pip install --upgrade deepface

# use the target size of FER2013 or FER+
plt.imshow(DeepFace.extract_faces(vid1.asnumpy()[50], grayscale=True, align=False)[0]['face'], cmap='gray')
#target_size=(48,48)

result = []
for i in range(vid1.shape[0]):
    res = DeepFace.analyze(vid1.asnumpy()[i], actions=['emotion'])[0]
    temp = {i: j for i,j in res['emotion'].items() if j>70}
    if len(temp)>0:
        result.append(temp)
        print(temp)

pd.DataFrame(result).max().sort_values(), pd.DataFrame(result).count().sort_values(), filepath













import random
import shutil

# Step 2: Define paths for source and destination folders
#source_folder = '/content/drive/MyDrive/Dessertation/videos'
#destination_folder = '/content/drive/MyDrive/Dessertation/selected_videos'

# Step 3: Ensure destination folder exists; doesn't modify original folder
#os.makedirs(destination_folder, exist_ok=True)

# Step 4: List all video files in source folder, keeping originals untouched
#all_videos = [f for f in os.listdir(source_folder) if f.endswith('.flv')]

# Step 5: Define emotion categories to sample from
#emotion_categories = ["ANG", "DIS", "FEA", "HAP", "NEU", "SAD"]

# Step 6: Set target samples per emotion (5 in this case for testing)
#target_samples_per_emotion = 100

# Step 6: Initialize storage for selected videos
#selected_videos = []  # Ensure this variable is defined before being used


# Debug: Print sample filenames to inspect their format
#print("Sample filenames for debugging:")
#for file in all_videos[:10]:  # Print the first 10 filenames
    #print(file)

# Step 7: Randomly select videos per emotion
#for emotion in emotion_categories:
    # Normalize filenames and emotion labels to uppercase for flexible matching
    #emotion_videos = [f for f in all_videos if emotion in f.upper()]

    # Debug: Check how many videos were detected for this emotion
    #print(f"Emotion: {emotion}, Detected files: {len(emotion_videos)}")

    #if not emotion_videos:
        #print(f"No videos found for emotion: {emotion}")
        #continue

# Randomly sample videos for the current emotion
#sampled_videos = random.sample(emotion_videos, min(target_samples_per_emotion, len(emotion_videos)))

    # Add sampled videos to the list
#selected_videos.extend(sampled_videos)

# Step 8: Copy sampled videos to destination folder without modifying originals
#for video in selected_videos:
#    src_path = os.path.join(source_folder, video)
 #   dest_path = os.path.join(destination_folder, video)
  #  shutil.copy(src_path, dest_path)#

# Step 9: Print summary of selected videos
#print(f"Selected {len(selected_videos)} videos for analysis, stored safely in the destination folder:")
#for video in selected_videos:
    #print(video)

import os
import random
import shutil

# Step 1: Define paths for source and destination folders
source_folder = '/content/drive/MyDrive/Dessertation/videos'
destination_folder = '/content/drive/MyDrive/Dessertation/selected_videos'

# Step 2: Ensure destination folder exists
os.makedirs(destination_folder, exist_ok=True)

# Step 3: List all video files in source folder
all_videos = [f for f in os.listdir(source_folder) if f.endswith('.flv')]

# Step 4: Define substrings to match emotions in filenames
emotion_substrings = ["ANG", "DIS", "FEA", "HAP", "NEU", "SAD"]

# Step 5: Set target samples per emotion
target_samples_per_emotion = 100

# Step 6: Initialize storage for selected videos
selected_videos = []

# Debug: Print sample filenames to inspect their format
print("Sample filenames for debugging:")
for file in all_videos[:10]:  # Print the first 10 filenames
    print(file)

# Step 7: Randomly select videos per emotion substring
for emotion_substring in emotion_substrings:
    # Filter files by substring
    emotion_videos = [f for f in all_videos if emotion_substring in f.upper()]

    # Debug: Check how many videos were detected for this substring
    print(f"Emotion substring: {emotion_substring}, Detected files: {len(emotion_videos)}")

    if not emotion_videos:
        print(f"No videos found for substring: {emotion_substring}")
        continue

    # Randomly sample videos for the current substring
    sampled_videos = random.sample(emotion_videos, min(target_samples_per_emotion, len(emotion_videos)))

    # Add sampled videos to the list
    selected_videos.extend(sampled_videos)

    # Copy sampled videos to destination folder
    for video in sampled_videos:
        src_path = os.path.join(source_folder, video)
        dest_path = os.path.join(destination_folder, video)
        shutil.copy(src_path, dest_path)

# Step 8: Print summary of selected videos
print(f"Selected {len(selected_videos)} videos for analysis, stored in the destination folder:")
for video in selected_videos[:10]:  # Print the first 10 selected videos
    print(video)

folder1_path = '/content/drive/MyDrive/Dessertation/selected_videos'
# List all video files in the directory
video1_files = [f for f in os.listdir(folder1_path) if f.lower().endswith(('mp4','flv','mov'))]

print(f"Total number of video files: {len(video1_files)}")

# List all video files in the directory
video1_files = [f for f in os.listdir(folder1_path) if f.lower().endswith(('mp4','flv','mov'))]

print(f"Total number of video files: {len(video1_files)}")

# Assuming the naming convention: ActorID_Scenario_Emotion_Intensity.extension
# Example: 1001_DFA_ANG_XX.flv
data1_records = []
for f in video1_files:
    # Remove extension and split by underscore
    base1_name = os.path.splitext(f)[0]
    parts1 = base1_name.split('_')

    # Make sure we have the right number of parts
    if len(parts1) == 4:
        actor_id, scenario, emotion, intensity = parts1
    else:
        # If naming differs, adjust parsing logic accordingly
        actor_id = parts1[0] if len(parts) > 0 else None
        scenario = parts1[1] if len(parts) > 1 else None
        emotion = parts1[2] if len(parts) > 2 else None
        intensity = parts1[3] if len(parts) > 3 else None

    data1_records.append({
        'filename': f,
        'actor_id': actor_id,
        'scenario': scenario,
        'emotion': emotion,
        'intensity': intensity
    })

# Create a DataFrame
df1 = pd.DataFrame(data1_records)
df1.head()

# @title emotion

from matplotlib import pyplot as plt
import seaborn as sns
df1.groupby('emotion').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

import cv2
import os

# Path to your video folder
folder_path = '/content/drive/MyDrive/Dessertation/selected_videos'
output_folder = '/content/drive/MyDrive/Dessertation/frames'  # Folder to save extracted frames
os.makedirs(output_folder, exist_ok=True)  # Create the folder if it doesn't exist

# Set the interval (e.g., extract a frame every 30 frames)
frame_interval = 30

# Load Haar Cascade for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Loop through all video files
video_files = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.flv', '.mov'))]

for video_file in video_files:
    video_path = os.path.join(folder_path, video_file)
    cap = cv2.VideoCapture(video_path)

    frame_count = 0
    frame_number = 0  # To track frame numbers for saving

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Process every 'frame_interval' frame
        if frame_count % frame_interval == 0:
            # Detect faces in the frame
            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

            # Draw rectangles around faces
            for (x, y, w, h) in faces:
                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

            # Save the frame with detected faces
            frame_filename = f"{os.path.splitext(video_file)[0]}_frame{frame_number}.jpg"
            frame_path = os.path.join(output_folder, frame_filename)
            cv2.imwrite(frame_path, frame)
            frame_number += 1

        frame_count += 1

    cap.release()

print(f"Frames extracted and saved to {output_folder}")

from deepface import DeepFace

# Analyze a frame for emotion
analysis = DeepFace.analyze(img_path=frame_path, actions=['emotion'])
print(analysis)

from deepface import DeepFace
import cv2
from matplotlib import pyplot as plt

# Analyze the frame for emotion
analysis = DeepFace.analyze(img_path=frame_path, actions=['emotion'])[0]  # Get the first result if it's a list
dominant_emotion = analysis['dominant_emotion']

# Display the frame with emotion
frame = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2RGB)
plt.imshow(frame)
plt.axis('off')
plt.title(f"Emotion: {dominant_emotion}")
plt.show()



#Batch Processing of Videos:

import csv


# Define paths
video_folder = '/content/drive/MyDrive/Dessertation/selected_videos'  # Folder with video files
output_csv = '/content/drive/MyDrive/Dessertation/emotion_results.csv'  # Path to save results
frame_interval = 30  # Extract a frame every 30 frames

# Prepare results storage
results = []

# Get all video files
video_files = [f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.flv', '.mov'))]

for video_file in video_files:
    video_path = os.path.join(video_folder, video_file)
    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    print(f"Processing video: {video_file}")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Process every 'frame_interval' frame
        if frame_count % frame_interval == 0:
            # Save the frame temporarily
            temp_frame_path = 'temp_frame.jpg'
            cv2.imwrite(temp_frame_path, frame)

            try:
                # Analyze emotions using DeepFace
                analysis = DeepFace.analyze(img_path=temp_frame_path, actions=['emotion'])[0]
                dominant_emotion = analysis['dominant_emotion']
                emotion_score = analysis['emotion'][dominant_emotion]

                # Save result
                results.append({
                    'video': video_file,
                    'frame_number': frame_count,
                    'dominant_emotion': dominant_emotion,
                    'emotion_score': emotion_score
                })

                print(f"Frame {frame_count}: {dominant_emotion} ({emotion_score:.2f})")

            except Exception as e:
                print(f"Error analyzing frame {frame_count} of {video_file}: {e}")

        frame_count += 1

    cap.release()

# Save results to a CSV file
with open(output_csv, 'w', newline='') as csvfile:
    fieldnames = ['video', 'frame_number', 'dominant_emotion', 'emotion_score']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(results)

print(f"Emotion analysis results saved to {output_csv}")

# Load the CSV and filter low-confidence results
results_df = pd.read_csv('/content/drive/MyDrive/Dessertation/emotion_results.csv')
results_df = results_df[results_df['emotion_score'] >= 50]

# Data preprocessing

import os
import cv2
import librosa
import numpy as np
import pandas as pd
from deepface import DeepFace
from tqdm import tqdm

# Paths and Configurations
video_folder = '/content/drive/MyDrive/Dessertation/selected_videos'  # Folder containing videos
output1_csv = '/content/drive/MyDrive/Dessertation/preprocessed_data.csv'  # Path to save results
frame_interval = 30  # Process every 30 frames

# Emotion Mapping
emotion_mapping = {
    'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,
    'sad': 4, 'surprise': 5, 'neutral': 6
}

# Prepare Results Storage
results = []

# Function to extract facial features using DeepFace
def extract_facial_features(frame_path):
    try:
        analysis = DeepFace.analyze(img_path=frame_path, actions=['emotion'], enforce_detection=False)[0]
        dominant_emotion = analysis['dominant_emotion']
        emotion_scores = analysis['emotion']

        # Map dominant emotion to numeric
        numeric_emotion = emotion_mapping.get(dominant_emotion, -1)

        # Flatten emotion scores
        flattened_emotion_scores = [emotion_scores[emotion] for emotion in emotion_mapping.keys()]

        return numeric_emotion, flattened_emotion_scores
    except Exception as e:
        print(f"Error analyzing frame: {e}")
        return -1, [0] * len(emotion_mapping)

# Function to extract audio features using Librosa
def extract_audio_features(audio_path):
    try:
        y, sr = librosa.load(audio_path, sr=None, mono=True)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        return mfccs.mean(axis=1).tolist()
    except Exception as e:
        print(f"Error extracting audio features: {e}")
        return [0] * 13

# Process Each Video File
video_files = [f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.flv', '.mov'))]

for video_file in tqdm(video_files, desc="Processing Videos"):
    video_path = os.path.join(video_folder, video_file)
    cap = cv2.VideoCapture(video_path)

    frame_count = 0
    frame_results = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Process every 'frame_interval' frame
        if frame_count % frame_interval == 0:
            temp_frame_path = 'temp_frame.jpg'
            cv2.imwrite(temp_frame_path, frame)

            numeric_emotion, flattened_emotion_scores = extract_facial_features(temp_frame_path)

            if numeric_emotion != -1:
                frame_results.append({
                    'video': video_file,
                    'frame_number': frame_count,
                    'numeric_emotion': numeric_emotion,
                    'emotion_scores': flattened_emotion_scores
                })

        frame_count += 1

    cap.release()

# Extract audio features
audio_features = extract_audio_features(video_path)

# Merge frame and audio results
for frame_result in frame_results:
        frame_result['audio_features'] = audio_features
        frame_result['combined_features'] = frame_result['emotion_scores'] + audio_features
        results.append(frame_result)

# Save Results to CSV
results_df = pd.DataFrame(results)
results_df.to_csv(output1_csv, index=False)

print(f"Preprocessed data with numeric outputs saved to {output1_csv}")

import os
import cv2
import librosa
import numpy as np
import pandas as pd
from deepface import DeepFace
from tqdm import tqdm

# Paths and Configurations
video_folder = '/content/drive/MyDrive/Dessertation/selected_videos'  # Folder containing videos
output1_csv = '/content/drive/MyDrive/Dessertation/preprocessed_data.csv'  # Path to save results
frame_interval = 30  # Process every 30 frames

# Emotion Mapping based on filename substrings
emotion_mapping = {
    'ANG': 0, 'DIS': 1, 'FEA': 2, 'HAP': 3, 'SAD': 4, 'NEU': 5
}

# Prepare Results Storage
results = []

# Function to extract facial features using DeepFace
def extract_facial_features(frame_path):
    try:
        analysis = DeepFace.analyze(img_path=frame_path, actions=['emotion'], enforce_detection=False)[0]
        dominant_emotion = analysis['dominant_emotion']
        emotion_scores = analysis['emotion']

        # Map dominant emotion to numeric
        numeric_emotion = emotion_mapping.get(dominant_emotion.upper(), -1)

        # Flatten emotion scores
        flattened_emotion_scores = [emotion_scores.get(emotion.lower(), 0) for emotion in emotion_mapping.keys()]

        return numeric_emotion, flattened_emotion_scores
    except Exception as e:
        print(f"Error analyzing frame: {e}")
        return -1, [0] * len(emotion_mapping)

# Function to extract audio features using Librosa
def extract_audio_features(audio_path):
    try:
        y, sr = librosa.load(audio_path, sr=None, mono=True)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        return mfccs.mean(axis=1).tolist()
    except Exception as e:
        print(f"Error extracting audio features: {e}")
        return [0] * 13

# Map emotion from filename
def map_emotion_from_filename(filename):
    for emotion, code in emotion_mapping.items():
        if f"_{emotion}_" in filename.upper():
            return code
    return -1

# Process Each Video File
video_files = [f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.flv', '.mov'))]

for video_file in tqdm(video_files, desc="Processing Videos"):
    video_path = os.path.join(video_folder, video_file)
    cap = cv2.VideoCapture(video_path)

    frame_count = 0
    frame_results = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Process every 'frame_interval' frame
        if frame_count % frame_interval == 0:
            temp_frame_path = 'temp_frame.jpg'
            cv2.imwrite(temp_frame_path, frame)

            numeric_emotion = map_emotion_from_filename(video_file)
            if numeric_emotion == -1:
                print(f"Skipped video with no matching emotion: {video_file}")
                continue

            emotion_numeric, flattened_emotion_scores = extract_facial_features(temp_frame_path)

            if emotion_numeric != -1:
                frame_results.append({
                    'video': video_file,
                    'frame_number': frame_count,
                    'numeric_emotion': emotion_numeric,
                    'emotion_scores': flattened_emotion_scores
                })

        frame_count += 1

    cap.release()

    # Extract audio features for the video
    audio_features = extract_audio_features(video_path)

    # Merge frame and audio results
    for frame_result in frame_results:
        frame_result['audio_features'] = audio_features
        frame_result['combined_features'] = frame_result['emotion_scores'] + audio_features
        results.append(frame_result)

# Save Results to CSV
results_df = pd.DataFrame(results)
results_df.to_csv(output1_csv, index=False)

print(f"Preprocessed data with numeric outputs saved to {output1_csv}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Updated emotion mapping based on substrings in filenames
emotion_mapping = {
    'ANG': 0, 'DIS': 1, 'FEA': 2, 'HAP': 3, 'SAD': 4, 'NEU': 5
}

# Path to the preprocessed data CSV
preprocessed_data_csv = '/content/drive/MyDrive/Dessertation/preprocessed_data.csv'

# Load preprocessed data
def load_preprocessed_data(csv_path):
    data = pd.read_csv(csv_path)

    # Map emotion labels from filenames
    data['numeric_emotion'] = data['video'].apply(lambda x: map_emotion_from_filename(x))
    data = data[data['numeric_emotion'] >= 0]  # Remove rows where emotion mapping failed

    # Convert combined features from string to numerical array
    data['combined_features'] = data['combined_features'].apply(eval)

    X = np.array(data['combined_features'].tolist())
    y = np.array(data['numeric_emotion'])
    return X, y

# Function to map emotion from filenames
def map_emotion_from_filename(filename):
    for emotion, code in emotion_mapping.items():
        if f"_{emotion}_" in filename.upper():
            return code
    return -1  # Return -1 if no emotion substring is found





# Visualize class distribution
def visualize_class_distribution(y, title="Class Distribution"):
    unique, counts = np.unique(y, return_counts=True)
    plt.bar(unique, counts, tick_label=[key for key, val in emotion_mapping.items() if val in unique])
    plt.title(title)
    plt.xlabel("Emotion")
    plt.ylabel("Frequency")
    plt.show()

# Split data into train, validation, and test sets
def split_data(X, y, test_size=0.3, val_size=0.5, random_state=42):
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=random_state)
    return X_train, X_val, X_test, y_train, y_val, y_test

# Main script
if __name__ == "__main__":
    # Load data
    X, y = load_preprocessed_data(preprocessed_data_csv)
    # Visualize overall class distribution
    print("Overall Class Distribution:")
    visualize_class_distribution(y)

    # Split data
    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)

    # Visualize splits
    print("Training Class Distribution:")
    visualize_class_distribution(y_train, title="Training Set Class Distribution")
    print("Validation Class Distribution:")
    visualize_class_distribution(y_val, title="Validation Set Class Distribution")
    print("Test Class Distribution:")
    visualize_class_distribution(y_test, title="Test Set Class Distribution")





import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import classification_report, accuracy_score

# Define the model architecture
def create_model(input_shape, num_classes):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_shape,)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Train the model
def train_model(X_train, y_train, X_val, y_val, num_classes, epochs=20, batch_size=32):
    input_shape = X_train.shape[1]
    model = create_model(input_shape, num_classes)

    # Train the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1
    )
    return model, history

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    predicted_classes = predictions.argmax(axis=1)

    print("Accuracy on Test Data:", accuracy_score(y_test, predicted_classes))
    print("\nClassification Report:\n", classification_report(y_test, predicted_classes))

# Main script
if __name__ == "__main__":
    # Ensure the data is loaded and split
    num_classes = len(np.unique(y_train))  # Get the number of emotion classes

    # Train the model
    print("Training the model...")
    model, history = train_model(X_train, y_train, X_val, y_val, num_classes)

    # Evaluate the model
    print("\nEvaluating the model on the test set...")
    evaluate_model(model, X_test, y_test)

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Define the model architecture
def create_model(input_shape, num_classes):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.01)),
        Dropout(0.4),
        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
        Dropout(0.4),
        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
        Dropout(0.4),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Train the model
def train_model(X_train, y_train, X_val, y_val, num_classes, epochs=50, batch_size=32):
    input_shape = X_train.shape[1]
    model = create_model(input_shape, num_classes)

    # Callbacks for early stopping and learning rate reduction
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

    # Train the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping, lr_scheduler],
        verbose=1
    )
    return model, history

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    predicted_classes = predictions.argmax(axis=1)

    print("Accuracy on Test Data:", accuracy_score(y_test, predicted_classes))
    print("\nClassification Report:\n", classification_report(y_test, predicted_classes))

# Standardize the features
def standardize_data(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)
    return X_train, X_val, X_test

# Main script
if __name__ == "__main__":
    # Ensure the data is loaded and split
    num_classes = len(np.unique(y_train))  # Get the number of emotion classes

    # Standardize the data
    print("Standardizing data...")
    X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)

    # Train the model
    print("Training the model...")
    model, history = train_model(X_train, y_train, X_val, y_val, num_classes)

    # Evaluate the model
    print("\nEvaluating the model on the test set...")
    evaluate_model(model, X_test, y_test)

    # Plot training and validation accuracy
    import matplotlib.pyplot as plt
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    # Plot training and validation loss
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Define the model architecture
def create_model(input_shape, num_classes):
    model = Sequential([
        Dense(512, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.01)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu', kernel_regularizer=l2(0.01)),
        BatchNormalization(),
        Dropout(0.4),
        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
        BatchNormalization(),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Train the model
def train_model(X_train, y_train, X_val, y_val, num_classes, epochs=50, batch_size=32):
    input_shape = X_train.shape[1]
    model = create_model(input_shape, num_classes)

    # Callbacks for early stopping and learning rate reduction
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

    # Train the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping, lr_scheduler],
        verbose=1
    )
    return model, history

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    predicted_classes = predictions.argmax(axis=1)

    print("Accuracy on Test Data:", accuracy_score(y_test, predicted_classes))
    print("\nClassification Report:\n", classification_report(y_test, predicted_classes))

# Standardize the features
def standardize_data(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)
    return X_train, X_val, X_test

# Main script
if __name__ == "__main__":
    # Ensure the data is loaded and split
    num_classes = len(np.unique(y_train))  # Get the number of emotion classes

    # Standardize the data
    print("Standardizing data...")
    X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)

    # Train the model
    print("Training the model...")
    model, history = train_model(X_train, y_train, X_val, y_val, num_classes)

    # Evaluate the model
    print("\nEvaluating the model on the test set...")
    evaluate_model(model, X_test, y_test)

    # Plot training and validation accuracy
    import matplotlib.pyplot as plt
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    # Plot training and validation loss
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

# Compute ROC curve and ROC area for each class
def plot_roc_curve(model, X_test, y_test, num_classes):
    # Binarize the labels
    y_test_binarized = label_binarize(y_test, classes=list(range(num_classes)))
    y_score = model.predict(X_test)

    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Plot ROC curve for each class
    plt.figure(figsize=(10, 8))
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple'])
    for i, color in zip(range(num_classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2,
                 label=f'Class {i} (area = {roc_auc[i]:0.2f})')

    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

# Call the function to plot the ROC curve
print("\nPlotting ROC Curve...")
plot_roc_curve(model, X_test, y_test, num_classes)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt
import numpy as np

# Assuming `y_test` is the true labels and `y_pred` is the predicted labels from RFCNN

def generate_confusion_matrix(y_test, y_pred, emotion_mapping):
    # Generate confusion matrix
    cm = confusion_matrix(y_test, y_pred, labels=list(emotion_mapping.values()))

    # Display confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(emotion_mapping.keys()))
    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)
    plt.title("Confusion Matrix - RFCNN")
    plt.show()

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=list(emotion_mapping.keys())))

# Example usage
emotion_mapping = {
    'Anger': 0,
    'Disgust': 1,
    'Fear': 2,
    'Happiness': 3,
    'Sadness': 4,
    'Neutral': 5
}

# Replace these with your actual `y_test` and `y_pred` values
y_test = np.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5])  # True labels
y_pred = np.array([0, 1, 2, 3, 4, 4, 0, 0, 2, 3, 3, 5])  # Predicted labels from RFCNN

generate_confusion_matrix(y_test, y_pred, emotion_mapping)

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

# Random Forest Classifier
def train_random_forest(X_train, y_train, X_test, y_test):
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
    print("\nRandom Forest Classification Report:\n", classification_report(y_test, y_pred))

# XGBoost Classifier
def train_xgboost(X_train, y_train, X_test, y_test):
    model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print("XGBoost Accuracy:", accuracy_score(y_test, y_pred))
    print("\nXGBoost Classification Report:\n", classification_report(y_test, y_pred))

# Support Vector Machine Classifier
def train_svm(X_train, y_train, X_test, y_test):
    model = SVC(kernel='rbf', probability=True, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print("SVM Accuracy:", accuracy_score(y_test, y_pred))
    print("\nSVM Classification Report:\n", classification_report(y_test, y_pred))

# Standardize the features
def standardize_data(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)
    return X_train, X_val, X_test

# Main script
if __name__ == "__main__":
    # Ensure the data is loaded and split
    # Replace the placeholders below with your actual dataset loading logic
    X, y = load_preprocessed_data(preprocessed_data_csv)  # Assume this function loads your dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Standardize the data
    print("Standardizing data...")
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train and evaluate Random Forest
    print("Training Random Forest...")
    train_random_forest(X_train, y_train, X_test, y_test)

    # Train and evaluate XGBoost
    print("Training XGBoost...")
    train_xgboost(X_train, y_train, X_test, y_test)

    # Train and evaluate SVM
    print("Training SVM...")
    train_svm(X_train, y_train, X_test, y_test)



import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


# Replace with the actual data if available
y_test = np.random.randint(0, 6, 100)  # Replace with actual test labels
y_pred_rf = np.random.randint(0, 6, 100)  # Replace with Random Forest predictions
y_pred_xgb = np.random.randint(0, 6, 100)  # Replace with XGBoost predictions
y_pred_svm = np.random.randint(0, 6, 100)  # Replace with SVM predictions

# Define a function to plot confusion matrices
def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ANG', 'DIS', 'FEA', 'HAP', 'SAD', 'NEU'])
    disp.plot(cmap='viridis')
    plt.title(title)
    plt.show()

# Plot confusion matrices for each model
plot_confusion_matrix(y_test, y_pred_rf, "Confusion Matrix: Random Forest")
plot_confusion_matrix(y_test, y_pred_xgb, "Confusion Matrix: XGBoost")
plot_confusion_matrix(y_test, y_pred_svm, "Confusion Matrix: SVM")

import numpy as np
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Random Forest Classifier with ROC Curve
def train_random_forest_with_roc(X_train, y_train, X_test, y_test):
    # Train Random Forest Classifier
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Predict the probabilities
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
    print("\nRandom Forest Classification Report:\n", classification_report(y_test, y_pred))

    # Binarize the labels for ROC calculation
    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
    n_classes = y_test_bin.shape[1]

    # Compute ROC curve and AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}

    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Plot ROC Curve
    plt.figure()
    colors = ['aqua', 'orange', 'blue', 'red', 'green', 'purple']
    for i, color in enumerate(colors[:n_classes]):
        plt.plot(fpr[i], tpr[i], color=color, lw=2,
                 label=f"Class {i} (area = {roc_auc[i]:.2f})")

    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Receiver Operating Characteristic (ROC) Curve - Random Forest")
    plt.legend(loc="lower right")
    plt.grid()
    plt.show()

# Example Usage
# Replace X_train, X_test, y_train, y_test with your dataset
# train_random_forest_with_roc(X_train, y_train, X_test, y_test)



import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report

# Simulated metrics for illustration (replace with actual metrics from your models)
# Replace these values with actual computed metrics
accuracy_scores = {
    "Random Forest": 0.85,
    "XGBoost": 0.87,
    "SVM": 0.83
}

f1_scores = {
    "Random Forest": 0.84,
    "XGBoost": 0.86,
    "SVM": 0.82
}

precision_scores = {
    "Random Forest": 0.83,
    "XGBoost": 0.88,
    "SVM": 0.81
}

recall_scores = {
    "Random Forest": 0.85,
    "XGBoost": 0.87,
    "SVM": 0.83
}

# Plot comparison graphs
def plot_comparison(metrics, title, ylabel):
    models = metrics.keys()
    scores = metrics.values()

    plt.figure(figsize=(8, 5))
    plt.bar(models, scores, color='skyblue')
    plt.title(title)
    plt.ylabel(ylabel)
    plt.ylim(0, 1)  # Assuming metric scores are between 0 and 1
    plt.xticks(rotation=45)
    plt.show()

# Accuracy comparison
plot_comparison(accuracy_scores, "Model Comparison: Accuracy", "Accuracy")

# F1-Score comparison
plot_comparison(f1_scores, "Model Comparison: F1-Score", "F1-Score")

# Precision comparison
plot_comparison(precision_scores, "Model Comparison: Precision", "Precision")

# Recall comparison
plot_comparison(recall_scores, "Model Comparison: Recall", "Recall")



import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Replace with your actual test labels and predictions
y_test = np.random.randint(0, 6, 100)  # Simulated true labels
y_pred_rf = np.random.randint(0, 6, 100)  # Random Forest predictions
y_pred_xgb = np.random.randint(0, 6, 100)  # XGBoost predictions
y_pred_svm = np.random.randint(0, 6, 100)  # SVM predictions

# Define a function to plot confusion matrices side by side
def plot_confusion_matrices(y_true, preds, titles, labels):
    fig, axes = plt.subplots(1, len(preds), figsize=(15, 5))
    for ax, y_pred, title in zip(axes, preds, titles):
        cm = confusion_matrix(y_true, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
        disp.plot(ax=ax, cmap='viridis', colorbar=False)
        ax.set_title(title)
    plt.tight_layout()
    plt.show()

# Model titles and predictions
titles = ["Random Forest", "XGBoost", "SVM"]
predictions = [y_pred_rf, y_pred_xgb, y_pred_svm]

# Emotion labels
labels = ['ANG', 'DIS', 'FEA', 'HAP', 'SAD', 'NEU']

# Plot comparison
plot_confusion_matrices(y_test, predictions, titles, labels)

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

# Define the LSTM model architecture
def create_lstm_model(input_shape, num_classes):
    model = Sequential([
        LSTM(128, return_sequences=True, input_shape=input_shape, activation='relu'),
        Dropout(0.3),
        LSTM(64, return_sequences=False, activation='relu'),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Train the LSTM model
def train_lstm_model(X_train, y_train, X_val, y_val, num_classes, epochs=50, batch_size=32):
    input_shape = (X_train.shape[1], X_train.shape[2])  # Time steps and features
    model = create_lstm_model(input_shape, num_classes)

    # Callbacks for early stopping and learning rate reduction
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

    # Train the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping, lr_scheduler],
        verbose=1
    )
    return model, history

# Evaluate the LSTM model
def evaluate_lstm_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    predicted_classes = predictions.argmax(axis=1)

    print("Accuracy on Test Data:", accuracy_score(y_test, predicted_classes))
    print("\nClassification Report:\n", classification_report(y_test, predicted_classes))

# Pad features to make them divisible by timesteps
def pad_features(X, timesteps):
    feature_count = X.shape[1]
    padding_required = timesteps - (feature_count % timesteps)
    if padding_required != timesteps:
        X = np.pad(X, ((0, 0), (0, padding_required)), mode='constant', constant_values=0)
    return X

# Reshape data for LSTM
def reshape_for_lstm(X, timesteps):
    num_samples = X.shape[0]
    num_features = X.shape[1] // timesteps
    return X.reshape(num_samples, timesteps, num_features)

# Standardize the features
def standardize_data(X_train, X_val, X_test):
    scaler = StandardScaler()
    X_train_shape = X_train.shape
    X_val_shape = X_val.shape
    X_test_shape = X_test.shape

    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train_shape)
    X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val_shape)
    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test_shape)

    return X_train, X_val, X_test

# Main script
if __name__ == "__main__":
    # Ensure the data is loaded and split
    X, y = load_preprocessed_data(preprocessed_data_csv)  # Assume this function loads your dataset

    # Define timesteps for LSTM
    timesteps = 30

    # Pad and reshape the data for LSTM
    X = pad_features(X, timesteps)
    try:
        X = reshape_for_lstm(X, timesteps)
    except ValueError as e:
        print(e)
        exit()

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)

    # Standardize the data
    print("Standardizing data...")
    X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)

    # Train the LSTM model
    print("Training LSTM model...")
    num_classes = len(np.unique(y))
    model, history = train_lstm_model(X_train, y_train, X_val, y_val, num_classes)

    # Evaluate the LSTM model
    print("\nEvaluating the LSTM model on the test set...")
    evaluate_lstm_model(model, X_test, y_test)

    # Plot training and validation accuracy
    import matplotlib.pyplot as plt
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    # Plot training and validation loss
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()



import os
import cv2
import librosa
import numpy as np
import pandas as pd
from deepface import DeepFace
from tqdm import tqdm

# Paths and Configurations
video_folder = '/content/drive/MyDrive/Dessertation/selected_videos'  # Folder containing videos
facial_csv = '/content/drive/MyDrive/Dessertation/facial_features.csv'  # Path to save facial features
speech_csv = '/content/drive/MyDrive/Dessertation/speech_features.csv'  # Path to save speech features
frame_interval = 30  # Process every 30 frames

# Emotion Mapping based on filename substrings
emotion_mapping = {
    'ANG': 0, 'DIS': 1, 'FEA': 2, 'HAP': 3, 'SAD': 4, 'NEU': 5
}

# Prepare Results Storage
facial_results = []
speech_results = []

# Function to extract facial features using DeepFace
def extract_facial_features(frame_path):
    try:
        analysis = DeepFace.analyze(img_path=frame_path, actions=['emotion'], enforce_detection=False)[0]
        dominant_emotion = analysis['dominant_emotion']
        emotion_scores = analysis['emotion']

        # Map dominant emotion to numeric
        numeric_emotion = emotion_mapping.get(dominant_emotion.upper(), -1)

        # Flatten emotion scores
        flattened_emotion_scores = [emotion_scores.get(emotion.lower(), 0) for emotion in emotion_mapping.keys()]

        return numeric_emotion, flattened_emotion_scores
    except Exception as e:
        print(f"Error analyzing frame: {e}")
        return -1, [0] * len(emotion_mapping)

# Function to extract audio features using Librosa
def extract_audio_features(audio_path):
    try:
        y, sr = librosa.load(audio_path, sr=None, mono=True)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        return mfccs.mean(axis=1).tolist()
    except Exception as e:
        print(f"Error extracting audio features: {e}")
        return [0] * 13

# Map emotion from filename
def map_emotion_from_filename(filename):
    for emotion, code in emotion_mapping.items():
        if f"_{emotion}_" in filename.upper():
            return code
    return -1

# Process Each Video File
video_files = [f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.flv', '.mov'))]

for video_file in tqdm(video_files, desc="Processing Videos"):
    video_path = os.path.join(video_folder, video_file)
    cap = cv2.VideoCapture(video_path)

    frame_count = 0
    frame_results = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Process every 'frame_interval' frame
        if frame_count % frame_interval == 0:
            temp_frame_path = 'temp_frame.jpg'
            cv2.imwrite(temp_frame_path, frame)

            numeric_emotion = map_emotion_from_filename(video_file)
            if numeric_emotion == -1:
                print(f"Skipped video with no matching emotion: {video_file}")
                continue

            emotion_numeric, flattened_emotion_scores = extract_facial_features(temp_frame_path)

            if emotion_numeric != -1:
                facial_results.append({
                    'video': video_file,
                    'frame_number': frame_count,
                    'numeric_emotion': emotion_numeric,
                    'facial_features': flattened_emotion_scores
                })

        frame_count += 1

    cap.release()

    # Extract audio features for the video
    audio_features = extract_audio_features(video_path)
    if audio_features:
        speech_results.append({
            'video': video_file,
            'audio_features': audio_features,
            'numeric_emotion': map_emotion_from_filename(video_file)
        })

# Save Facial Features to CSV
facial_df = pd.DataFrame(facial_results)
facial_df.to_csv(facial_csv, index=False)
print(f"Facial features saved to {facial_csv}")

# Save Speech Features to CSV
speech_df = pd.DataFrame(speech_results)
speech_df.to_csv(speech_csv, index=False)
print(f"Speech features saved to {speech_csv}")

import os
import cv2
import librosa
import numpy as np
import pandas as pd
from deepface import DeepFace
from tqdm import tqdm
import matplotlib.pyplot as plt

# Paths and Configurations
video_folder = '/content/drive/MyDrive/Dessertation/selected_videos'  # Folder containing videos
facial_csv = '/content/drive/MyDrive/Dessertation/facial_features.csv'  # Path to save facial features
speech_csv = '/content/drive/MyDrive/Dessertation/speech_features.csv'  # Path to save speech features
frame_interval = 30  # Process every 30 frames

# Emotion Mapping based on filename substrings
emotion_mapping = {
    'ANG': 0, 'DIS': 1, 'FEA': 2, 'HAP': 3, 'SAD': 4, 'NEU': 5
}

# Prepare Results Storage
facial_results = []
speech_results = []

# Function to extract facial features using DeepFace
def extract_facial_features(frame_path):
    try:
        analysis = DeepFace.analyze(img_path=frame_path, actions=['emotion'], enforce_detection=False)[0]
        dominant_emotion = analysis['dominant_emotion']
        emotion_scores = analysis['emotion']

        # Map dominant emotion to numeric
        numeric_emotion = emotion_mapping.get(dominant_emotion[:3].upper(), -1)

        # Flatten emotion scores
        flattened_emotion_scores = [emotion_scores.get(emotion.lower(), 0) for emotion in emotion_mapping.keys()]

        return numeric_emotion, flattened_emotion_scores
    except Exception as e:
        print(f"Error analyzing frame: {e}")
        return -1, [0] * len(emotion_mapping)

# Function to extract audio features using Librosa
def extract_audio_features(audio_path):
    try:
        y, sr = librosa.load(audio_path, sr=None, mono=True)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        return mfccs.mean(axis=1).tolist()
    except Exception as e:
        print(f"Error extracting audio features: {e}")
        return [0] * 13

# Map emotion from filename
def map_emotion_from_filename(filename):
    for emotion, code in emotion_mapping.items():
        if f"_{emotion}_" in filename.upper():
            return code
    return -1

# Process Each Video File
video_files = [f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.flv', '.mov'))]

for video_file in tqdm(video_files, desc="Processing Videos"):
    video_path = os.path.join(video_folder, video_file)
    cap = cv2.VideoCapture(video_path)

    frame_count = 0
    frame_results = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Process every 'frame_interval' frame
        if frame_count % frame_interval == 0:
            temp_frame_path = 'temp_frame.jpg'
            cv2.imwrite(temp_frame_path, frame)

            numeric_emotion = map_emotion_from_filename(video_file)
            if numeric_emotion == -1:
                print(f"Skipped video with no matching emotion: {video_file}")
                continue

            emotion_numeric, flattened_emotion_scores = extract_facial_features(temp_frame_path)

            if emotion_numeric != -1:
                facial_results.append({
                    'video': video_file,
                    'frame_number': frame_count,
                    'numeric_emotion': numeric_emotion,
                    'facial_features': flattened_emotion_scores
                })

        frame_count += 1

    cap.release()

    # Extract audio features for the video
    audio_features = extract_audio_features(video_path)
    if audio_features:
        speech_results.append({
            'video': video_file,
            'audio_features': audio_features,
            'numeric_emotion': map_emotion_from_filename(video_file)
        })

# Save Facial Features to CSV
facial_df = pd.DataFrame(facial_results)
facial_df.to_csv(facial_csv, index=False)
print(f"Facial features saved to {facial_csv}")

# Save Speech Features to CSV
speech_df = pd.DataFrame(speech_results)
speech_df.to_csv(speech_csv, index=False)
print(f"Speech features saved to {speech_csv}")

# Visualization: Class Distribution
facial_emotions = [result['numeric_emotion'] for result in facial_results]
speech_emotions = [result['numeric_emotion'] for result in speech_results]

# Plot class distribution for facial features
plt.figure(figsize=(10, 5))
plt.hist(facial_emotions, bins=len(emotion_mapping), alpha=0.7, label='Facial Features', color='blue', rwidth=0.8)
plt.xticks(ticks=range(len(emotion_mapping)), labels=emotion_mapping.keys())
plt.title('Class Distribution for Facial Features')
plt.xlabel('Emotion')
plt.ylabel('Frequency')
plt.legend()
plt.show()

# Plot class distribution for speech features
plt.figure(figsize=(10, 5))
plt.hist(speech_emotions, bins=len(emotion_mapping), alpha=0.7, label='Speech Features', color='green', rwidth=0.8)
plt.xticks(ticks=range(len(emotion_mapping)), labels=emotion_mapping.keys())
plt.title('Class Distribution for Speech Features')
plt.xlabel('Emotion')
plt.ylabel('Frequency')
plt.legend()
plt.show()

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import LSTM, Dense, Dropout, Concatenate
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import classification_report, accuracy_score

# Paths to preprocessed data
facial_csv = '/content/drive/MyDrive/Dessertation/facial_features.csv'
speech_csv = '/content/drive/MyDrive/Dessertation/speech_features.csv'

# Load preprocessed data
def load_data(facial_csv, speech_csv):
    facial_data = pd.read_csv(facial_csv)
    speech_data = pd.read_csv(speech_csv)

    # Align datasets on 'video'
    merged_data = pd.merge(facial_data, speech_data, on='video', suffixes=('_facial', '_speech'))

    # Extract features and labels
    X_facial = np.array(merged_data['facial_features'].apply(eval).tolist())
    X_speech = np.array(merged_data['audio_features'].apply(eval).tolist())
    y = np.array(merged_data['numeric_emotion_facial'])  # Assuming labels are consistent

    # Pad features to make them divisible by timesteps
    def pad_features(X, timesteps):
        feature_count = X.shape[1]
        padding_required = timesteps - (feature_count % timesteps)
        if padding_required != timesteps:
            X = np.pad(X, ((0, 0), (0, padding_required)), mode='constant', constant_values=0)
        return X

    timesteps = 30  # Adjust based on your data
    X_facial = pad_features(X_facial, timesteps)
    X_speech = pad_features(X_speech, timesteps)

    # Reshape features for LSTM
    def reshape_for_lstm(X, timesteps):
        num_samples = X.shape[0]
        num_features = X.shape[1] // timesteps
        return X.reshape(num_samples, timesteps, num_features)

    X_facial = reshape_for_lstm(X_facial, timesteps)
    X_speech = reshape_for_lstm(X_speech, timesteps)

    return X_facial, X_speech, y

# Create multi-input model
def create_multi_input_model(input_shape_facial, input_shape_speech, num_classes):
    # Facial input branch
    facial_input = Input(shape=input_shape_facial, name="Facial_Input")
    x_facial = LSTM(64, return_sequences=True, activation='relu')(facial_input)
    x_facial = LSTM(32, activation='relu')(x_facial)
    x_facial = Dropout(0.3)(x_facial)

    # Speech input branch
    speech_input = Input(shape=input_shape_speech, name="Speech_Input")
    x_speech = LSTM(64, return_sequences=True, activation='relu')(speech_input)
    x_speech = LSTM(32, activation='relu')(x_speech)
    x_speech = Dropout(0.3)(x_speech)

    # Combine branches
    combined = Concatenate()([x_facial, x_speech])
    x = Dense(64, activation='relu')(combined)
    x = Dropout(0.3)(x)
    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=[facial_input, speech_input], outputs=output)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Main script
if __name__ == "__main__":
    # Load data
    X_facial, X_speech, y = load_data(facial_csv, speech_csv)

    # Split data
    X_facial_train, X_facial_test, X_speech_train, X_speech_test, y_train, y_test = train_test_split(
        X_facial, X_speech, y, test_size=0.2, random_state=42, stratify=y
    )
    X_facial_train, X_facial_val, X_speech_train, X_speech_val, y_train, y_val = train_test_split(
        X_facial_train, X_speech_train, y_train, test_size=0.25, random_state=42, stratify=y_train
    )

    # Define input shapes
    input_shape_facial = (X_facial_train.shape[1], X_facial_train.shape[2])
    input_shape_speech = (X_speech_train.shape[1], X_speech_train.shape[2])

    # Create model
    num_classes = len(np.unique(y))
    model = create_multi_input_model(input_shape_facial, input_shape_speech, num_classes)

    # Callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

    # Train model
    history = model.fit(
        [X_facial_train, X_speech_train], y_train,
        validation_data=([X_facial_val, X_speech_val], y_val),
        epochs=50,
        batch_size=32,
        callbacks=[early_stopping, lr_scheduler],
        verbose=1
    )

    # Evaluate model
    y_pred = model.predict([X_facial_test, X_speech_test]).argmax(axis=1)
    print("Accuracy on Test Data:", accuracy_score(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

    # Save model
    model.save('/content/drive/MyDrive/Dessertation/multi_input_model.h5')

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, Concatenate, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import classification_report, accuracy_score

# Paths to preprocessed data
facial_csv = '/content/drive/MyDrive/Dessertation/facial_features.csv'
speech_csv = '/content/drive/MyDrive/Dessertation/speech_features.csv'

# Load preprocessed data
def load_data(facial_csv, speech_csv):
    facial_data = pd.read_csv(facial_csv)
    speech_data = pd.read_csv(speech_csv)

    # Align datasets on 'video'
    merged_data = pd.merge(facial_data, speech_data, on='video', suffixes=('_facial', '_speech'))

    # Extract features and labels
    X_facial = np.array(merged_data['facial_features'].apply(eval).tolist())
    X_speech = np.array(merged_data['audio_features'].apply(eval).tolist())
    y = np.array(merged_data['numeric_emotion_facial'])  # Assuming labels are consistent

    # Debug: Print shapes before reshaping
    print(f"Original X_facial shape: {X_facial.shape}")
    print(f"Original X_speech shape: {X_speech.shape}")

    # Handle low-dimensional facial data
    if X_facial.shape[1] <= 30:  # Low-dimensional data (e.g., emotion scores)
        X_facial = X_facial.reshape(X_facial.shape[0], X_facial.shape[1], 1)
    else:
        raise ValueError("Facial data is incompatible with expected input format for CNN-LSTM.")

    # Reshape speech data for LSTM
    timesteps = 30
    feature_size = max(1, X_speech.shape[1] // timesteps)  # Ensure feature_size >= 1
    if X_speech.shape[1] < timesteps * feature_size:
        print("Padding speech features to match timesteps.")
        padding_size = timesteps * feature_size - X_speech.shape[1]
        X_speech = np.hstack([X_speech, np.zeros((X_speech.shape[0], padding_size))])
    elif X_speech.shape[1] > timesteps * feature_size:
        print("Truncating speech features to match timesteps.")
        X_speech = X_speech[:, :timesteps * feature_size]
    X_speech = X_speech.reshape(-1, timesteps, feature_size)

    # Debug: Print shapes after reshaping
    print(f"Reshaped X_facial shape: {X_facial.shape}")
    print(f"Reshaped X_speech shape: {X_speech.shape}")

    return X_facial, X_speech, y

# Create CNN-LSTM model
def create_cnn_lstm_model(input_shape_facial, input_shape_speech, num_classes):
    # Facial input branch (LSTM for low-dimensional data)
    facial_input = Input(shape=input_shape_facial, name="Facial_Input")
    x_facial = LSTM(64, return_sequences=True, activation='relu')(facial_input)
    x_facial = LSTM(32, activation='relu')(x_facial)
    x_facial = Dropout(0.3)(x_facial)

    # Speech input branch (LSTM)
    speech_input = Input(shape=input_shape_speech, name="Speech_Input")
    x_speech = LSTM(64, return_sequences=True, activation='relu')(speech_input)
    x_speech = LSTM(32, activation='relu')(x_speech)
    x_speech = Dropout(0.3)(x_speech)

    # Combine branches
    combined = Concatenate()([x_facial, x_speech])
    x = Dense(64, activation='relu')(combined)
    x = Dropout(0.3)(x)
    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=[facial_input, speech_input], outputs=output)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Main script
if __name__ == "__main__":
    # Load data
    X_facial, X_speech, y = load_data(facial_csv, speech_csv)

    # Split data
    X_facial_train, X_facial_test, X_speech_train, X_speech_test, y_train, y_test = train_test_split(
        X_facial, X_speech, y, test_size=0.2, random_state=42, stratify=y
    )
    X_facial_train, X_facial_val, X_speech_train, X_speech_val, y_train, y_val = train_test_split(
        X_facial_train, X_speech_train, y_train, test_size=0.25, random_state=42, stratify=y_train
    )

    # Define input shapes
    input_shape_facial = (X_facial_train.shape[1], X_facial_train.shape[2])
    input_shape_speech = (X_speech_train.shape[1], X_speech_train.shape[2])

    # Create model
    num_classes = len(np.unique(y))
    model = create_cnn_lstm_model(input_shape_facial, input_shape_speech, num_classes)

    # Callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

    # Train model
    history = model.fit(
        [X_facial_train, X_speech_train], y_train,
        validation_data=([X_facial_val, X_speech_val], y_val),
        epochs=50,
        batch_size=32,
        callbacks=[early_stopping, lr_scheduler],
        verbose=1
    )

    # Evaluate model
    y_pred = model.predict([X_facial_test, X_speech_test]).argmax(axis=1)
    print("Accuracy on Test Data:", accuracy_score(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

    # Save model
    model.save('/content/drive/MyDrive/Dessertation/cnn_lstm_model.h5')

import xgboost as xgb
from sklearn.metrics import classification_report, accuracy_score
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Paths to data
facial_csv = '/content/drive/MyDrive/Dessertation/facial_features.csv'
speech_csv = '/content/drive/MyDrive/Dessertation/speech_features.csv'

# Load preprocessed data
def load_data(facial_csv, speech_csv):
    facial_data = pd.read_csv(facial_csv)
    speech_data = pd.read_csv(speech_csv)

    # Align datasets on 'video'
    merged_data = pd.merge(facial_data, speech_data, on='video', suffixes=('_facial', '_speech'))

    # Extract features and labels
    X_facial = np.array(merged_data['facial_features'].apply(eval).tolist())
    X_speech = np.array(merged_data['audio_features'].apply(eval).tolist())
    y = np.array(merged_data['numeric_emotion_facial'])  # Assuming labels are consistent

    # Concatenate facial and speech features
    X = np.hstack([X_facial, X_speech])
    return X, y

# Load data
X, y = load_data(facial_csv, speech_csv)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train XGBoost model
xgb_model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y)),
    eval_metric='mlogloss',
    use_label_encoder=False
)
xgb_model.fit(X_train, y_train)

# Evaluate model
y_pred = xgb_model.predict(X_test)
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred))
print("\nXGBoost Classification Report:\n", classification_report(y_test, y_pred))

# Feedback system
def feedback_system(facial_input, speech_input):
    # Combine features
    input_features = np.hstack([facial_input, speech_input])

    # Predict emotion
    emotion_label = xgb_model.predict([input_features])[0]

    # Map to mental health category
    emotion_mapping = {
        0: "Healthy",
        1: "Stressed",
        2: "Anxious",
        3: "Balanced",
        4: "Depressed",
        5: "Frustrated"
    }
    feedback = emotion_mapping.get(emotion_label, "Unknown")

    return feedback

# Example feedback usage
facial_example = X_test[0][:len(X_test[0])//2]
speech_example = X_test[0][len(X_test[0])//2:]
print("Feedback:", feedback_system(facial_example, speech_example))

import xgboost as xgb
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt

# Paths to data
facial_csv = '/content/drive/MyDrive/Dessertation/facial_features.csv'
speech_csv = '/content/drive/MyDrive/Dessertation/speech_features.csv'

# Load preprocessed data
def load_data(facial_csv, speech_csv):
    facial_data = pd.read_csv(facial_csv)
    speech_data = pd.read_csv(speech_csv)

    # Align datasets on 'video'
    merged_data = pd.merge(facial_data, speech_data, on='video', suffixes=('_facial', '_speech'))

    # Extract features and labels
    X_facial = np.array(merged_data['facial_features'].apply(eval).tolist())
    X_speech = np.array(merged_data['audio_features'].apply(eval).tolist())
    y = np.array(merged_data['numeric_emotion_facial'])  # Assuming labels are consistent

    # Concatenate facial and speech features
    X = np.hstack([X_facial, X_speech])
    return X, y

# Load data
X, y = load_data(facial_csv, speech_csv)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# XGBoost model with evaluation
xgb_model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y)),
    eval_metric='mlogloss',
    use_label_encoder=False
)

# Train model with evaluation metrics
eval_set = [(X_train, y_train), (X_test, y_test)]
xgb_model.fit(X_train, y_train, eval_set=eval_set, verbose=True)

# Predictions
y_pred = xgb_model.predict(X_test)
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred))
print("\nXGBoost Classification Report:\n", classification_report(y_test, y_pred))

# Plot accuracy
results = xgb_model.evals_result()

plt.figure(figsize=(10, 6))
plt.plot(results['validation_0']['mlogloss'], label='Train')
plt.plot(results['validation_1']['mlogloss'], label='Validation')
plt.xlabel('Iterations')
plt.ylabel('Log Loss')
plt.title('XGBoost Training and Validation Log Loss')
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import xgboost as xgb
import joblib

# Paths to data
facial_csv = '/content/drive/MyDrive/Dessertation/facial_features.csv'
speech_csv = '/content/drive/MyDrive/Dessertation/speech_features.csv'

# Load preprocessed data
def load_data(facial_csv, speech_csv):
    facial_data = pd.read_csv(facial_csv)
    speech_data = pd.read_csv(speech_csv)

    # Align datas\ets on 'video'
    merged_data = pd.merge(facial_data, speech_data, on='video', suffixes=('_facial', '_speech'))

    # Extract features and labels
    X_facial = np.array(merged_data['facial_features'].apply(eval).tolist())
    X_speech = np.array(merged_data['audio_features'].apply(eval).tolist())
    y = np.array(merged_data['numeric_emotion_facial'])

    # Concatenate facial and speech features
    X = np.hstack([X_facial, X_speech])
    return X, y, merged_data['video']

# Load data
X, y, video_names = load_data(facial_csv, speech_csv)

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test, video_train, video_test = train_test_split(X, y, video_names, test_size=0.2, random_state=42, stratify=y)

# Train XGBoost model
xgb_model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y)),
    eval_metric='mlogloss',
    use_label_encoder=False
)
xgb_model.fit(X_train, y_train)

# Save the trained model
joblib.dump(xgb_model, '/content/drive/MyDrive/Dessertation/xgboost_model.pkl')

# Feedback system
def feedback_system(facial_input, speech_input, video_name):
    # Combine features
    input_features = np.hstack([facial_input, speech_input])

    # Predict emotion
    emotion_label = xgb_model.predict([input_features])[0]

    # Map to mental health feedback
    emotion_mapping = {
        0: "Irritability Detected",
        1: "Annoyed",
        2: "High Anxiety",
        3: "Healthy",
        4: "Depression Warning",
        5: "Emotionally Stable"
    }
    feedback = emotion_mapping.get(emotion_label, "Unknown")

    # Map to suggestions
    suggestion_mapping = {
        0: "Hi! Let's take a deep breath and count to 10.... Feel better?",
        1: "Hi! You seem annoyed, let's hear your favourite song!",
        2: "Hi! I feel you are anxious, no need to worry, you are not alone!",
        3: "Hi! I am thrilled that you are healthy and happy!",
        4: "Hi...I am sorry you feel that way, we will work this out, now let's take a deep breath.",
        5: "Hi! Well let's keep going!"
    }
    suggestion = suggestion_mapping.get(emotion_label, "No suggestion available.")

    # Confidence score
    probabilities = xgb_model.predict_proba([input_features])[0]
    confidence = probabilities[emotion_label] * 100

    return feedback, confidence, video_name, suggestion

# Example usage
facial_example = X_test[0][:len(X_test[0]) // 2]
speech_example = X_test[0][len(X_test[0]) // 2:]
video_name = video_test.iloc[0]
feedback, confidence, video_name, suggestion = feedback_system(facial_example, speech_example, video_name)

print(f"Video Analyzed: {video_name}")
print(f"Feedback: {feedback}")
print(f"Confidence: {confidence:.2f}%")
print(f"Suggestion: {suggestion}")

# Visualize confidence distribution
import matplotlib.pyplot as plt
confidences = [xgb_model.predict_proba([X_test[i]])[0].max() * 100 for i in range(len(X_test))]
plt.hist(confidences, bins=10, alpha=0.7, color='blue')
plt.title("Confidence Score Distribution")
plt.xlabel("Confidence (%)")
plt.ylabel("Frequency")
plt.show()





